# -*- coding: utf-8 -*-
"""[Jonatas-Liberato] sistema-identificacao-notas-falsas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z2v0JGWAkNvlizmlJUqe55wMDoVMF051

# **SISTEMA DE IDENTIFICAÇÃO DE NOTAS FALSAS**

**O Problema:**

Um dos grandes problemas que atingem bancos e empresas de variados tamanhos. Para se ser uma noção, mais de 21 milhões em notas falsas foram apreendidas em 2021 segundo essa [reportagem](https://noticias.r7.com/brasilia/brasil-teve-quase-r-25-milhoes-em-notas-falsas-apreendidas-em-2021-09042022)

--

E para burlar os sistemas de segurança, os criminosos apresentam técnicas e métodos novos que dificultam e muito a identificação das cédulas falsas, principalmente **analisando visualmente**.

--

**O projeto:**

Utilizaremos um dataset do Github quem contém algumas features binárias que serão utilizadas para treinar nossa Machine Learning através **visão computacional**.

--

Devemos considerar que os dados já possuem transformações foram encontradas 4 características, como: 

- A **Variância** da imagem transformada em ondas
- A **Assimetria** da imagem transformada em ondas.
- A **Curtose** da imagem transformada em ondaletas
- A **Entropia** de imagem

--

**Insights:**

- Identificamos que o TARGET é a variável **'auth'**
- 0 = verdadeira e 1 = falsa

Dataset: [AQUI](https://raw.githubusercontent.com/amankharwal/Website-data/master/data_banknote_authentication.txt)

# 1. BIBLIOTECAS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

"""# 2. ANÁLISE EXPLORATÓRIA"""

# dataset
dataset = pd.read_csv('https://raw.githubusercontent.com/amankharwal/Website-data/master/data_banknote_authentication.txt', header = None)
dataset

"""**Renomeando as colunas**"""

dataset.columns = ['var', 'knew', 'curt', 'entr', 'auth']
dataset

# Tipos dos dados
dataset.info()

# Tamanho do dataset
dataset.shape

# Visualizando os dados e como se comportam em relação ao TARGET
# 0 = verdadeira e 1 = falsa
sns.pairplot(dataset, hue = 'auth')
plt.show()

"""**Conclusões**

* A distribuição da variância e da assimetria se mostram bem diferente, já para a curtose e a entropia parecem ser bem parecidos
* Em algumas características notamos certa correlação (curtose x assimetria)
* Algumas características parecem separar muito bem notas genuínas e falsas  (var x assimetria

**Balanceamento em relação ao TARGET**
"""

# Contando quantos registros tem em cada resultado do TARGET
target_count = dataset.auth.value_counts()
target_count

plt.figure(figsize = (10,6))
plt.title('Distribuição do Target(auth)', size = 18)
sns.countplot(x = dataset['auth'])
plt.annotate(s = target_count[0], xy = (-0.04,10 + target_count[0]), size = 14)
plt.annotate(s = target_count[0], xy = (0.96,10 + target_count[1]), size = 14)
plt.ylim(0, 900)
plt.show()

"""**Importante**

Apesar dos dados não estarem muito desbalanceados, é preciso sim realizar um melhoramento neste, pois usaremos classificação binária.

--

Aqui um ponto muito importante, como existem mais dados de "não falsas" que "falsas", a máquina preditiva pode interpretar e ter um viés para cédulas verdadeiras.

--

Para isso, utilizaremos o *smooth* do **oversample** para fazer uma **sobreamostragem**, usando dados sintéticos para ajudar no balanceamento ideal.

# 3. Pré-Processamento

Aqui faremos uma sbuamostragem. Ao invés de AUMENTAR na amostragem que tem menos, vamos DIMINUIR do que tem mais, através da estatística. Ambos ficaram em 610.
"""

subamostragem = target_count[0] - target_count[1]
dataset = dataset.sample(frac = 1, random_state = 42).sort_values(by = 'auth')
dataset = dataset[subamostragem:]
print(dataset['auth'].value_counts())

"""**Separando TARGET(y) das explicativas(x)**"""

x = dataset.loc[:, dataset.columns != 'auth']
y = dataset.loc[:, dataset.columns == 'auth']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)

"""**Verificando a escala dos dados**"""

x.describe()

"""**Conclusão:** Verificando que assimetria, variância, curtose e entropia estão em escalas diferentes. Exemplo: os valores máximos da curtose e da entropia estão bem diferentes.

**Padronizando as variáveis**
"""

scalar = StandardScaler()
scalar.fit(x_train)
x_train = scalar.transform(x_train)
x_test = scalar.transform(x_test)

# Agora ambos estão na mesma escala

"""# 4. Máquina Preditiva"""

# Treinando os dados de treino
machine = LogisticRegression(solver = 'lbfgs', random_state = 42, multi_class = 'auto')
machine.fit(x_train, y_train.values.ravel())

# Treinando os dados de teste
y_pred = np.array(machine.predict(x_test))

"""# 5. Avaliação"""

conf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred),
                        columns=["Pred.Negative", "Pred.Positive"],
                        index=['Act.Negative', "Act.Positive"])
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
accuracy = round((tn+tp)/(tn+fp+fn+tp), 4)
print(conf_mat)
print(f'\n Accuracy = {round(100*accuracy, 2)}%')

"""# Conclusão

Nosso modelo de regressão logística resultou em uma acurácia de ~98% e toda vez que identificou que uma cédula era falsa, ele acertou 100%.
"""

#Autor: Jonatas A. Liberato
#Ref: Eduardo Rocha